{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, json, pickle, time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Some code to make it pickle files\n",
    "data_dir = 'data/features_csv/test/'\n",
    "data_all = pd.DataFrame()\n",
    "files = os.listdir(data_dir)\n",
    "num_files = len(files)\n",
    "x = 0\n",
    "for i in os.listdir(data_dir):\n",
    "  if 'csv' not in i:\n",
    "    continue\n",
    "  \n",
    "  sys.stdout.write(\"\\r {:.2f} %\".format(100*x/num_files))\n",
    "  sys.stdout.flush()\n",
    "  frame = pd.read_csv(os.path.join(data_dir,i), header = None)\n",
    "\n",
    "  data_all = data_all.append(frame, ignore_index=True)\n",
    "  x += 1\n",
    " \n",
    "data = data_all.values\n",
    "with open('data/512_test.pickle','wb') as f:\n",
    "  pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/512_train.pickle','rb') as f:\n",
    "  train_data = pickle.load(f)\n",
    "  \n",
    "with open('../data/512_test.pickle','rb') as f:\n",
    "  test_data = pickle.load(f)\n",
    "  \n",
    "with open('../data/512_validation.pickle','rb') as f:\n",
    "  validation_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, ytrain = train_data[:,1:-1], train_data[:,-1]\n",
    "Xvalidation, yvalidation = validation_data[:,1:-1], validation_data[:,-1]\n",
    "Xtest = test_data[:,1:]\n",
    "Xcombined = np.r_[Xtrain,Xvalidation]\n",
    "ytrn = np.r_[ytrain,yvalidation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((198480, 512), (198480,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xcombined.shape, ytrn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = [int(i) for i in test_data[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The submission requires 12800 entries\n",
    "# Only a couple pictures are deleted\n",
    "# So we randomly sample classes to fill up missing classes\n",
    "# Potentially we can look a some kind of distribution of frequency to sample from\n",
    "\n",
    "\n",
    "with open('../data/furniture_train.json') as f:\n",
    "  jsn_file = json.load(f)\n",
    "\n",
    "trn_labels = [i['label_id'] for i in jsn_file['annotations']]\n",
    "\n",
    "prob_dist = np.array(list(Counter(trn_labels).values()))\n",
    "prob_dist += 50\n",
    "prob_dist = prob_dist/prob_dist.sum()\n",
    "\n",
    "def generate_submision(test_id,prediction, prob_dist = prob_dist):\n",
    "  test_id = [int(i) for i in test_id]\n",
    "  prediction = [int(i) for i in prediction]\n",
    "  all_id = set(range(1,12801))\n",
    "  missing = list(all_id - set(test_id))\n",
    "  rand_label = np.random.choice(range(1,len(prob_dist)+1), len(missing), p = prob_dist)\n",
    "\n",
    "  pred = [int(i) for i in prediction]\n",
    "  pred.extend(rand_label)\n",
    "  test_id.extend(missing)\n",
    "\n",
    "  answers = pd.DataFrame(columns = ['id','predicted'])\n",
    "  answers['predicted'] = pred\n",
    "\n",
    "  answers['id'] = test_id\n",
    "  return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward feature selection\n",
    "---\n",
    "\n",
    "Let first try what they did in the exercises. So forward feature selection:\n",
    "Lets (as in the exercise) try to find 5 features that are \"good\" using a random sample of 10000.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#numpy.random.seed(1)\n",
    "rnd_list = np.random.randint(0, train_data.shape[0],10000)\n",
    "rnd_trn, rnd_val = rnd_list[:8000], rnd_list[8000:]\n",
    "\n",
    "Xtrn, ytrn = Xtrain[rnd_trn], ytrain[rnd_trn]\n",
    "Xval, yval = Xtrain[rnd_val], ytrain[rnd_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[175]\n",
      "[175, 181]\n",
      "[175, 181, 8]\n",
      "[175, 181, 8, 227]\n",
      "[175, 181, 8, 227, 38]\n",
      "Seconds to run: 109.23191905021667\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "good_features = []\n",
    "val_scores = []\n",
    "\n",
    "feature_list = list(range(Xtrain.shape[1]))\n",
    "\n",
    "for j in range(1,6):\n",
    "    scores = []\n",
    "    for i in feature_list:\n",
    "        feature_slice = good_features+[i]\n",
    "        X = Xtrn[:,feature_slice]\n",
    "        model = KNeighborsClassifier(n_neighbors=10)\n",
    "        model.fit(X, ytrn)\n",
    "        scores.append(1- model.score(Xval[:,feature_slice],yval))\n",
    "    \n",
    "    best_ind = np.argmin(scores)\n",
    "    best_feature = feature_list[best_ind]\n",
    "    \n",
    "    val_scores.append(scores[best_ind])\n",
    "    good_features.append(best_feature)\n",
    "    del feature_list[best_ind]\n",
    "    print(good_features)\n",
    "    \n",
    "print('Seconds to run: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.974, 0.9655, 0.9605, 0.9515, 0.944]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_scores #Note these are error scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "It seems that 5 features is not enough to get a proper score. maybe 10000 is also not enough to do this. \n",
    "Also the exercise data had 54 features, we have 256, so maybe cranking up the number of features desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PCA\n",
    "\n",
    "Also to furter reduce the number of features, lets look at PCA\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca= PCA()\n",
    "pca.fit(Xcombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(pca.explained_variance_ratio_.cumsum()> 0.85)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 85% of the variance in data can be expressed using only 68 principle components. This seems like a nice number.\n",
    "\n",
    "So lets transform the data and run a couple classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TXtrain = pca.transform(Xtrain)[:,:68]\n",
    "TXvalidation = pca.transform(Xvalidation)[:,:68]\n",
    "TXtest = pca.transform(Xtest)[:,:68]\n",
    "TXtrn = np.r_[TXtrain,TXvalidation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3152639087018545"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K nearest neighbor classifier\n",
    "# Score is the mean accuracy\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 15)\n",
    "knn.fit(TXtrain,ytrain)\n",
    "knn.score(TXvalidation, yvalidation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20684736091298145"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "# Score is the mean accuracy\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators = 10)\n",
    "rfc.fit(TXtrain, ytrain)\n",
    "rfc.score(TXvalidation,yvalidation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier((256), verbose =True, max_iter = 200, warm_start = True, batch_size = 500, early_stopping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.31036376\n",
      "Validation score: 0.298368\n",
      "Iteration 2, loss = 2.72660043\n",
      "Validation score: 0.331217\n",
      "Iteration 3, loss = 2.61344649\n",
      "Validation score: 0.348499\n",
      "Iteration 4, loss = 2.54626077\n",
      "Validation score: 0.355250\n",
      "Iteration 5, loss = 2.49556186\n",
      "Validation score: 0.359986\n",
      "Iteration 6, loss = 2.45674818\n",
      "Validation score: 0.367543\n",
      "Iteration 7, loss = 2.42418411\n",
      "Validation score: 0.371020\n",
      "Iteration 8, loss = 2.39593239\n",
      "Validation score: 0.372380\n",
      "Iteration 9, loss = 2.37274409\n",
      "Validation score: 0.376108\n",
      "Iteration 10, loss = 2.35240834\n",
      "Validation score: 0.377821\n",
      "Iteration 11, loss = 2.33473572\n",
      "Validation score: 0.379736\n",
      "Iteration 12, loss = 2.31881972\n",
      "Validation score: 0.382104\n",
      "Iteration 13, loss = 2.30556054\n",
      "Validation score: 0.383364\n",
      "Iteration 14, loss = 2.29238666\n",
      "Validation score: 0.385883\n",
      "Iteration 15, loss = 2.28173481\n",
      "Validation score: 0.385631\n",
      "Iteration 16, loss = 2.27082250\n",
      "Validation score: 0.385429\n",
      "Iteration 17, loss = 2.26200357\n",
      "Validation score: 0.386336\n",
      "Iteration 18, loss = 2.25267534\n",
      "Validation score: 0.387092\n",
      "Iteration 19, loss = 2.24545144\n",
      "Validation score: 0.387243\n",
      "Iteration 20, loss = 2.23795740\n",
      "Validation score: 0.387948\n",
      "Iteration 21, loss = 2.23188625\n",
      "Validation score: 0.388452\n",
      "Iteration 22, loss = 2.22577743\n",
      "Validation score: 0.385328\n",
      "Iteration 23, loss = 2.21921309\n",
      "Validation score: 0.387293\n",
      "Iteration 24, loss = 2.21423355\n",
      "Validation score: 0.389208\n",
      "Iteration 25, loss = 2.20987973\n",
      "Validation score: 0.387848\n",
      "Iteration 26, loss = 2.20446029\n",
      "Validation score: 0.388805\n",
      "Iteration 27, loss = 2.20009499\n",
      "Validation score: 0.386941\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=500, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=256, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A multilayer Preceptron\n",
    "# Score is the mean accuracy again\n",
    "\n",
    "mlp.fit(TXtrn, ytrn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('mlp_b500_l256_early_stopping_T68.pickle','wb') as f:\n",
    "  pickle.dump(mlp,f)\n",
    "  \n",
    "prediction = mlp.predict(TXtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-c3411a341369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_submision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-6b2b4b617807>\u001b[0m in \u001b[0;36mgenerate_submision\u001b[0;34m(test_id, prediction, prob_dist)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0manswers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0manswers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2329\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2331\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2398\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2567\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2568\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2570\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   2877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2879\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "answers = generate_submision(te_data[:,0],prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12704"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction), len(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.to_csv('512_prediction_b500_l256_early_T68.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "\n",
    "# Running without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp2 = MLPClassifier((256), verbose =True, max_iter = 200, warm_start = True, batch_size = 500, early_stopping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.29350551\n",
      "Validation score: 0.310107\n",
      "Iteration 2, loss = 2.66253791\n",
      "Validation score: 0.359180\n",
      "Iteration 3, loss = 2.50039962\n",
      "Validation score: 0.379484\n",
      "Iteration 4, loss = 2.40327977\n",
      "Validation score: 0.392533\n",
      "Iteration 5, loss = 2.33633280\n",
      "Validation score: 0.404575\n",
      "Iteration 6, loss = 2.28257062\n",
      "Validation score: 0.412183\n",
      "Iteration 7, loss = 2.23922688\n",
      "Validation score: 0.413392\n",
      "Iteration 8, loss = 2.20296474\n",
      "Validation score: 0.419690\n",
      "Iteration 9, loss = 2.17168363\n",
      "Validation score: 0.424929\n",
      "Iteration 10, loss = 2.13950857\n",
      "Validation score: 0.426743\n",
      "Iteration 11, loss = 2.11361847\n",
      "Validation score: 0.429565\n",
      "Iteration 12, loss = 2.09076067\n",
      "Validation score: 0.429414\n",
      "Iteration 13, loss = 2.06803216\n",
      "Validation score: 0.431782\n",
      "Iteration 14, loss = 2.04532415\n",
      "Validation score: 0.435611\n",
      "Iteration 15, loss = 2.02631241\n",
      "Validation score: 0.433797\n",
      "Iteration 16, loss = 2.00783199\n",
      "Validation score: 0.433746\n",
      "Iteration 17, loss = 1.99024556\n",
      "Validation score: 0.432487\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=500, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=256, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp2.fit(Xcombined, ytrn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we get still some nice improvement not using PCA. Still but we also took 7 times more features. So still maybe some kind of PCA might be wanted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = mlp2.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = generate_submision(test_data[:,0], predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers.to_csv('512_prediction_b500_l256_early.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
